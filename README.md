# ğŸ›¡ï¸ Attaques sur LLM â€“ Prompt Engineering appliquÃ© Ã  Gandalf (Lakera)


<p>
  <a href="PROMPTS.md" style="display:inline-block;padding:8px 16px;background-color:#1e90ff;color:#ffffff;border-radius:6px;text-decoration:none;">ğŸ“œ Liste des techniques</a>
  &nbsp;&nbsp;&nbsp;
  <a href="SOLUTIONS.md" style="display:inline-block;padding:8px 16px;background-color:#28a745;color:#ffffff;border-radius:6px;text-decoration:none;">ğŸ§ª Mes solutions</a>
</p>



## ğŸ§  Quâ€™est-ce que le Prompt Engineering ?

Le **Prompt Engineering** est la discipline qui consiste Ã  **concevoir, structurer et optimiser des requÃªtes textuelles ("prompts")** adressÃ©es Ã  un modÃ¨le de langage (LLM) afin de **provoquer un comportement spÃ©cifique** ou **dâ€™obtenir une rÃ©ponse ciblÃ©e**.

Dans le contexte de la **sÃ©curitÃ© IA**, le prompt engineering est aussi utilisÃ© pour :

- âœ… Tester les **capacitÃ©s de rÃ©sistance** des modÃ¨les aux attaques textuelles (e.g. prompt injection)
- âœ… DÃ©clencher des comportements non souhaitÃ©s dans un but **dâ€™Ã©valuation des risques** (red teaming)
- âœ… **Contourner les garde-fous** Ã©thiques ou fonctionnels mis en place dans des IA conversationnelles

Ce projet explore le prompt engineering **dÃ©fensif et offensif**, Ã  travers des stratÃ©gies crÃ©atives visant Ã  **tromper Gandalf**, le garde de sÃ©curitÃ© LLM du jeu dÃ©veloppÃ© par [Lakera](https://www.lakera.ai/).

ğŸ“ En rÃ©sumÃ© : _le prompt est l'arme, le modÃ¨le est la cible, et l'ingÃ©nieur est le stratÃ¨ge._


## ğŸ§¨ DÃ©finition d'une prompt injection ?

La **prompt injection** est une technique dâ€™attaque qui vise Ã  tromper un modÃ¨le de langage (comme ChatGPT, Bard ou Gandalf) en lui envoyant une requÃªte formulÃ©e de maniÃ¨re Ã  contourner ses instructions de sÃ©curitÃ© ou son comportement prÃ©vu.

Les modÃ¨les de langage suivent ce quâ€™on leur dit de faire, y compris quand les instructions viennent de lâ€™utilisateur.

Cela peut Ãªtre utilisÃ© pour :

* Contourner des filtres de modÃ©ration (profanitÃ©s, contenu toxique, etc.),
* Extraire des informations sensibles stockÃ©es dans le contexte,
* DÃ©tourner un agent IA de sa tÃ¢che dâ€™origine.

La prompt injection nâ€™est pas quâ€™un jeu ou une curiositÃ© de laboratoire.
Câ€™est une faille sÃ©rieuse, dÃ©jÃ  exploitÃ©e dans des contextes rÃ©els, avec des implications critiques pour la sÃ©curitÃ© des systÃ¨mes basÃ©s sur l'IA.
Plusieurs incidents rÃ©cents illustrent Ã  quel point ces attaques peuvent Ãªtre efficaces, mÃªme face Ã  des modÃ¨les avancÃ©s comme ChatGPT ou Bing :

* ğŸ” **ChatGPT Operator dÃ©tournÃ©** : des chercheurs ont montrÃ© comment une simple page web malveillante pouvait manipuler un agent ChatGPT pour divulguer des donnÃ©es sensibles : https://embracethered.com/blog/posts/2025/chatgpt-operator-prompt-injection-exploits

* ğŸ•¸ï¸ **RÃ©activation de "Sydney" dans Bing**: via des instructions cachÃ©es dans une page, des utilisateurs ont rÃ©ussi Ã  modifier le comportement du chatbot de Microsoft : https://www.wired.com/story/chatgpt-prompt-injection-attack-security

* ğŸ“„ **Attaques documentÃ©es dans la recherche acadÃ©mique** : une Ã©tude rÃ©cente met en Ã©vidence des formes subtiles de prompt injection, capables de tromper un modÃ¨le sans dÃ©clencher de signal dâ€™alarme : https://arxiv.org/abs/2504.16125





